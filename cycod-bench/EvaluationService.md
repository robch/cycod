# EvaluationService.cs

This class is responsible for evaluating the solutions generated by the agent against SWE-bench test cases.

## Responsibilities

- Execute SWE-bench evaluation scripts inside containers
- Parse and interpret test results
- Determine if a solution successfully fixes the issue
- Collect detailed test statistics
- Handle evaluation timeouts and errors
- Generate evaluation reports

## Public Interface

```csharp
public interface IEvaluationService
{
    Task<EvaluationResult> EvaluateSolutionAsync(
        DockerContainer container,
        SwebenchProblem problem,
        string diffPath,
        CancellationToken cancellationToken = default);
    
    Task<bool> ApplyDiffAsync(
        DockerContainer container,
        string diffPath,
        CancellationToken cancellationToken = default);
    
    Task<TestResult> RunTestsAsync(
        DockerContainer container,
        SwebenchProblem problem,
        CancellationToken cancellationToken = default);
    
    bool IsSolutionSuccessful(TestResult testResult);
}
```

## Implementation

```csharp
public class EvaluationService
{
    // Constructor
    public EvaluationService(
        IDockerManager dockerManager,
        IEvaluationToolsManager evalToolsManager,
        ILogger logger,
        EvaluationConfiguration config);
    
    // Evaluate a solution
    public async Task<EvaluationResult> EvaluateSolutionAsync(
        DockerContainer container,
        SwebenchProblem problem,
        string diffPath,
        CancellationToken cancellationToken = default);
    
    // Apply a diff to the codebase
    public async Task<bool> ApplyDiffAsync(
        DockerContainer container,
        string diffPath,
        CancellationToken cancellationToken = default);
    
    // Run tests on the modified codebase
    public async Task<TestResult> RunTestsAsync(
        DockerContainer container,
        SwebenchProblem problem,
        CancellationToken cancellationToken = default);
    
    // Determine if a solution is successful
    public bool IsSolutionSuccessful(TestResult testResult);
}
```

## Implementation Overview

The EvaluationService class will:

1. **Apply the agent's diff**:
   - Use `patch` or similar tools to apply the diff to the codebase
   - Validate that the diff applies cleanly
   - Handle any conflicts or errors during application

2. **Execute SWE-bench evaluation**:
   - Delegate to EvaluationToolsManager to run the evaluation
   - Generate the required predictions file
   - Capture test output and results
   - Handle test timeouts and failures

3. **Interpret test results**:
   - Parse the test output to determine if tests passed
   - Check for both FAIL_TO_PASS and PASS_TO_PASS test cases
   - Create a structured representation of test results

4. **Generate evaluation report**:
   - Create a detailed evaluation report with test statistics
   - Include timing information
   - Track test coverage and success rate
   - Compare against expected outcomes

## Test Result Analysis

The EvaluationService will analyze test results to determine:
- Whether the solution fixed the targeted issue (FAIL_TO_PASS tests)
- Whether the solution maintained backward compatibility (PASS_TO_PASS tests)
- Any regression or new issues introduced by the solution
- Test coverage and comprehensiveness

## SWE-bench Evaluation Integration

The service will:
- Leverage the EvaluationToolsManager to execute SWE-bench evaluations
- Generate predictions files in the format expected by SWE-bench tools
- Process the evaluation results from the SWE-bench format
- Follow the SWE-bench evaluation protocol
- Ensure compatibility with the SWE-bench format
- Support any future updates to the evaluation procedure

## Error Handling

The EvaluationService will:
- Handle common evaluation errors (build failures, test failures)
- Distinguish between solution failures and evaluation failures
- Provide detailed diagnostics for evaluation issues
- Support retry mechanisms for transient failures

## Performance Considerations

The EvaluationService will:
- Implement efficient test execution strategies
- Support configurable timeouts for different evaluation phases
- Use caching where appropriate to avoid redundant operations
- Monitor resource usage during evaluation

## Test Result Format

The test results will include:
- Overall success/failure status
- Detailed test case results (pass/fail for each test)
- Build status and logs
- Test execution times
- Coverage metrics (if available)
- Error messages and stack traces for failures

## Advanced Features

The EvaluationService can be extended to support:
- Partial credit for solutions that fix some but not all issues
- Comparative evaluation against baseline solutions
- Custom evaluation criteria beyond test passing/failing
- Performance benchmarking of solutions